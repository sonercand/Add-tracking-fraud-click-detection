{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "> Data set has over 183 milion rows and it is impossible to process in a regular laptop at one go. Therefore processing completed in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_(column):\n",
    "    column =np.log(column)\n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_sample):\n",
    "    columns = data_sample.columns\n",
    "    if 'click_id' in columns:\n",
    "        test_set=1\n",
    "    else:\n",
    "        test_set=0\n",
    "    data_sample = data_sample.sort_values(by=['click_time'])  \n",
    "    data_sample['day']=pd.to_datetime(data_sample.click_time).dt.day.astype('int')\n",
    "    data_sample['hour']=pd.to_datetime(data_sample.click_time).dt.hour.astype('int')\n",
    "    data_sample['min']=pd.to_datetime(data_sample.click_time).dt.minute.astype('int')\n",
    "   \n",
    "    #app_counts\n",
    "    app_count = data_sample[['ip','app','hour','day']].groupby(['ip','hour','day']).app.nunique()\n",
    "    app_count = app_count.reset_index()\n",
    "    app_count.columns = ['ip','hour','day','app_count']\n",
    "    app_count['app_count'] = norm_(app_count['app_count'])\n",
    "    merged = pd.merge(data_sample, app_count, how='left', on = ['ip','hour','day'])\n",
    "    del app_count\n",
    "    del data_sample\n",
    "    \n",
    "    # number of given app seen in tht hour\n",
    "    app_count = merged[['ip','app','hour','day']].groupby(['app','hour','day']).count()\n",
    "    app_count = app_count.reset_index()\n",
    "    app_count.columns = ['app','hour','day','apps']\n",
    "    app_count['apps'] = norm_(app_count['apps'])\n",
    "    merged = pd.merge(merged, app_count, how='left', on = ['app','hour','day'])\n",
    "    del app_count\n",
    "   \n",
    "    # app min\n",
    "    \n",
    "    app_count = merged[['ip','app','hour','day','min']].groupby(['ip','hour','day','min']).app.nunique()\n",
    "    app_count = app_count.reset_index()\n",
    "    app_count.columns = ['ip','hour','day','min','app_count_min']\n",
    "    app_count['app_count_min'] = norm_(app_count['app_count_min'])\n",
    "    merged = pd.merge(merged, app_count, how='left', on = ['ip','hour','day','min'])\n",
    "    del app_count\n",
    "    \n",
    "    \n",
    "    #device counts\n",
    "    device_count = merged[['ip','device','hour','day']].groupby(['ip','hour','day']).device.nunique()\n",
    "    device_count = device_count.reset_index()\n",
    "    device_count.columns = ['ip','hour','day','device_count']\n",
    "    device_count['device_count'] = norm_(device_count['device_count'])\n",
    "    merged = pd.merge(merged, device_count, how='left', on = ['ip','hour','day'])\n",
    "    del device_count\n",
    "    \n",
    "    # num devices seen that hour\n",
    "    device_count = merged[['ip','device','hour','day']].groupby(['device','hour','day']).count()\n",
    "    device_count = device_count.reset_index()\n",
    "    device_count.columns = ['device','hour','day','devices']\n",
    "    device_count['devices'] = norm_(device_count['devices'])\n",
    "    merged = pd.merge(merged, device_count, how='left', on = ['device','hour','day'])\n",
    "    del device_count\n",
    "    \n",
    "    #device min\n",
    "    device_count = merged[['ip','device','hour','day','min']].groupby(['ip','hour','day','min']).device.nunique()\n",
    "    device_count = device_count.reset_index()\n",
    "    device_count.columns = ['ip','hour','day','min','device_count_min']\n",
    "    device_count['device_count_min'] = norm_(device_count['device_count_min'])\n",
    "    merged = pd.merge(merged, device_count, how='left', on = ['ip','hour','day','min'])\n",
    "    del device_count\n",
    "    \n",
    "    #os_counts\n",
    "    os_count = merged[['ip','os','hour','day']].groupby(['ip','hour','day']).os.nunique()\n",
    "    os_count = os_count.reset_index()\n",
    "    os_count.columns = ['ip','hour','day','os_count']\n",
    "    os_count['os_count'] = norm_(os_count['os_count'])\n",
    "    merged = pd.merge(merged, os_count, how='left', on = ['ip','hour','day'])\n",
    "    del os_count\n",
    "    \n",
    "    # os seen that hour\n",
    "    os_count = merged[['ip','os','hour','day']].groupby(['os','hour','day']).count()\n",
    "    os_count = os_count.reset_index()\n",
    "    os_count.columns = ['os','hour','day','oss']\n",
    "    os_count['oss'] = norm_(os_count['oss'])\n",
    "    merged = pd.merge(merged, os_count, how='left', on = ['os','hour','day'])\n",
    "    del os_count\n",
    "    \n",
    "    #os_counts min\n",
    "    os_count = merged[['ip','os','hour','day','min']].groupby(['ip','hour','day','min']).os.nunique()\n",
    "    os_count = os_count.reset_index()\n",
    "    os_count.columns = ['ip','hour','day','min','os_count_min']\n",
    "    os_count['os_count_min'] = norm_(os_count['os_count_min'])\n",
    "    merged = pd.merge(merged, os_count, how='left', on = ['ip','hour','day','min'])\n",
    "    del os_count\n",
    "\n",
    "    #channel count\n",
    "    channel_count = merged[['ip','channel','hour','day']].groupby(['ip','hour','day']).channel.nunique()\n",
    "    channel_count = channel_count.reset_index()\n",
    "    channel_count.columns = ['ip','hour','day','channel_count']\n",
    "    channel_count['channel_count'] = norm_(channel_count['channel_count'])\n",
    "    merged = pd.merge(merged, channel_count, how='left', on = ['ip','hour','day'])\n",
    "    del channel_count\n",
    "    \n",
    "    # channel seen tht hour\n",
    "    channel_count = merged[['ip','channel','hour','day']].groupby(['channel','hour','day']).count()\n",
    "    channel_count = channel_count.reset_index()\n",
    "    channel_count.columns = ['channel','hour','day','channels']\n",
    "    channel_count['channels'] = norm_(channel_count['channels'])\n",
    "    merged = pd.merge(merged, channel_count, how='left', on = ['channel','hour','day'])\n",
    "    del channel_count\n",
    "    \n",
    "    #channel min\n",
    "    channel_count = merged[['ip','channel','hour','day','min']].groupby(['ip','hour','day','min']).channel.nunique()\n",
    "    channel_count = channel_count.reset_index()\n",
    "    channel_count.columns = ['ip','hour','day','min','channel_count_min']\n",
    "    channel_count['channel_count_min'] = norm_(channel_count['channel_count_min'])\n",
    "    merged = pd.merge(merged, channel_count, how='left', on = ['ip','hour','day','min'])\n",
    "    del channel_count\n",
    "    \n",
    "    # app device os channel combo count\n",
    "    adoc = merged[['ip','app','device','os','channel']].groupby(['app','device','os','channel']).count()\n",
    "    adoc = adoc.reset_index()\n",
    "    adoc.columns = ['app','device','os','channel','adoc']\n",
    "    adoc['adoc'] = norm_(adoc['adoc'])\n",
    "    merged = pd.merge(merged,adoc,how='left',on=['app','device','os','channel'])\n",
    "    del adoc\n",
    "    \n",
    "    # app device os channel combo count min\n",
    "    adoc = merged[['ip','app','device','os','channel','day','hour','min']].groupby(['app','device','os',\n",
    "                                                                                    'channel','day','hour','min']).count()\n",
    "    adoc = adoc.reset_index()\n",
    "    adoc.columns = ['app','device','os','channel','day','hour','min','adoc_min']\n",
    "    adoc['adoc_min'] = norm_(adoc['adoc_min'])\n",
    "    merged = pd.merge(merged,adoc,how='left',on=['app','device','os','channel','day','hour','min'])\n",
    "    del adoc\n",
    "    \n",
    "    # click counts\n",
    "    click_count = merged[['ip','channel','hour','day']].groupby(['ip','hour','day']).count()\n",
    "    click_count = click_count.reset_index()\n",
    "    click_count.columns = ['ip','hour','day','click_count']\n",
    "    click_count['click_count']=norm_(click_count['click_count'])\n",
    "    merged = pd.merge(merged, click_count, how='left', on = ['ip','hour','day'])\n",
    "    del click_count\n",
    "    \n",
    "    # click counts MIN\n",
    "    click_count = merged[['ip','channel','hour','day','min']].groupby(['ip','hour','day','min']).count()\n",
    "    click_count = click_count.reset_index()\n",
    "    click_count.columns = ['ip','hour','day','min','click_count_min']\n",
    "    click_count['click_count_min'] = norm_(click_count['click_count_min'])\n",
    "    merged = pd.merge(merged, click_count, how='left', on = ['ip','hour','day','min'])\n",
    "    del click_count\n",
    "    \n",
    "    #click counts cumcount min\n",
    "    merged['click_count_cumsum_min'] = merged[['ip','channel','hour','day','min']].groupby(['ip','day','hour','min']).channel.cumcount()\n",
    "    \n",
    "\n",
    "    #click counts cumsum hour\n",
    "    merged['click_count_cumsum'] = merged[['ip','channel','hour','day']].groupby(['ip','day','hour']).channel.cumcount()\n",
    "    \n",
    "    if test_set==0:\n",
    "        features = merged[['ip','app','device','os','channel','hour','min','day','app_count','device_count','os_count',\n",
    "                           'channel_count','click_count','apps','devices','oss',\n",
    "                           'channels','adoc','app_count_min','device_count_min','os_count_min','channel_count_min',\n",
    "                           'click_count_min','adoc_min','click_count_cumsum','click_count_cumsum_min',\n",
    "                           'is_attributed']]\n",
    "    else:\n",
    "        features = merged[['click_id','ip','app','device','os','channel','hour','min','day','app_count','device_count','os_count',\n",
    "                           'channel_count','click_count','apps','devices','oss',\n",
    "                           'channels','adoc','app_count_min','device_count_min','os_count_min','channel_count_min',\n",
    "                           'click_count_min','adoc_min','click_count_cumsum','click_count_cumsum_min',\n",
    "                           'is_attributed']]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implement feature extraction on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11851\n",
      "1693\n",
      "2\n",
      "11865\n",
      "1695\n",
      "3\n",
      "12019\n",
      "1717\n",
      "4\n",
      "12859\n",
      "1837\n",
      "5\n",
      "14084\n",
      "2012\n",
      "6\n",
      "16177\n",
      "2311\n",
      "7\n",
      "14070\n",
      "2010\n",
      "8\n",
      "13055\n",
      "1865\n",
      "9\n",
      "11837\n",
      "1691\n",
      "10\n",
      "13202\n",
      "1886\n",
      "11\n",
      "15932\n",
      "2276\n",
      "12\n",
      "16779\n",
      "2397\n",
      "13\n",
      "18032\n",
      "2576\n",
      "14\n",
      "21028\n",
      "3004\n",
      "15\n",
      "21630\n",
      "3090\n",
      "16\n",
      "22876\n",
      "3268\n",
      "17\n",
      "25151\n",
      "3593\n",
      "18\n",
      "24815\n",
      "3545\n",
      "19\n",
      "25151\n",
      "3593\n",
      "20\n",
      "23835\n",
      "3405\n",
      "21\n",
      "23765\n",
      "3395\n",
      "22\n",
      "19824\n",
      "2832\n",
      "23\n",
      "16681\n",
      "2383\n",
      "24\n",
      "16079\n",
      "2297\n",
      "25\n",
      "15260\n",
      "2180\n",
      "26\n",
      "16352\n",
      "2336\n",
      "27\n",
      "15631\n",
      "2233\n",
      "28\n",
      "17521\n",
      "2503\n",
      "29\n",
      "17255\n",
      "2465\n",
      "30\n",
      "18445\n",
      "2635\n",
      "31\n",
      "18053\n",
      "2579\n",
      "32\n",
      "18074\n",
      "2582\n",
      "33\n",
      "19271\n",
      "2753\n",
      "34\n",
      "19075\n",
      "2725\n",
      "35\n",
      "18592\n",
      "2656\n",
      "36\n",
      "18410\n",
      "2630\n",
      "37\n",
      "22869\n",
      "3267\n",
      "38\n",
      "22540\n",
      "3220\n",
      "39\n",
      "21714\n",
      "3102\n",
      "40\n",
      "20979\n",
      "2997\n",
      "41\n",
      "19516\n",
      "2788\n",
      "42\n",
      "17297\n",
      "2471\n",
      "43\n",
      "17409\n",
      "2487\n",
      "44\n",
      "16926\n",
      "2418\n",
      "45\n",
      "18487\n",
      "2641\n",
      "46\n",
      "20545\n",
      "2935\n",
      "47\n",
      "21147\n",
      "3021\n",
      "48\n",
      "21840\n",
      "3120\n",
      "49\n",
      "21266\n",
      "3038\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10 ** 6\n",
    "k=0\n",
    "for chunk in pd.read_csv('train.csv', chunksize=chunksize):\n",
    "    chunk = process_data(chunk)\n",
    "    df1 = chunk[chunk.is_attributed==1]\n",
    "   # df3 = df1.copy()\n",
    "    n = len(df1)\n",
    "    df2 = chunk[chunk.is_attributed==0]\n",
    "    df2 = df2.sample(n*6)\n",
    "    frames = [df1,df2]\n",
    "    con_f = pd.concat(frames)\n",
    "    con_f =con_f.sample(frac=1.0)\n",
    "    data_sample = con_f\n",
    "    del con_f\n",
    "    del df1\n",
    "    del df2\n",
    "    del frames\n",
    "    \n",
    "    k+=1\n",
    "    print(k)\n",
    "    print(len(data_sample))\n",
    "    print(len(data_sample[data_sample.is_attributed==1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_sample.to_pickle('.\\processed_train_parts1\\part'+str(k)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract features test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18790469\n"
     ]
    }
   ],
   "source": [
    "#data_sample = pd.read_csv('test.csv')\n",
    "#data_sample['is_attributed']=2\n",
    "#print(len(data_sample))\n",
    "#features = process_data(data_sample)\n",
    "#features['click_id']=data_sample['click_id']    \n",
    "    \n",
    "#features.to_csv('.\\processed_test_parts\\processed_test_data_2.csv')\n",
    "\n",
    "chunksize = 10 ** 6\n",
    "k=0\n",
    "for chunk in pd.read_csv('test.csv', chunksize=chunksize):\n",
    "    chunk['is_attributed'] = 2\n",
    "    features = process_data(chunk)\n",
    "    k+=1\n",
    "    print(k)\n",
    "    print(len(data_sample))\n",
    "    features.to_pickle('.\\processed_test_parts\\part'+str(k)+'.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
